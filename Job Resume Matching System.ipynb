{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "def upload_resume():\n",
    "    global uploaded_filename\n",
    "    \n",
    "    # Function to handle uploading of resume\n",
    "    filename = filedialog.askopenfilename(initialdir=\"./\", title=\"Select file\", filetypes=((\"PDF files\", \"*.pdf\"), (\"All files\", \"*.*\")))\n",
    "    if filename:\n",
    "        print(\"Uploaded file:\", filename)\n",
    "        # Save the filename to a variable for further use\n",
    "        uploaded_filename = os.path.basename(filename)\n",
    "        \n",
    "        # Save the uploaded file to the \"./dataset/\" folder\n",
    "        destination = os.path.join(\"./dataset/\", uploaded_filename)\n",
    "        shutil.copyfile(filename, destination)\n",
    "        print(\"File saved to:\", destination)\n",
    "        \n",
    "        # Mock dataset of job positions\n",
    "        job_positions = []\n",
    "        for i in range(len(docID)):\n",
    "            job_positions.append({\"title\": positions[i], \"details\": jd[i]})\n",
    "        \n",
    "        # Update message label\n",
    "        message_label.config(text=\"Resume uploaded. Processing resume...\")\n",
    "        \n",
    "        # After 3-4 seconds, close the window\n",
    "        root.after(3000, root.destroy)\n",
    "        \n",
    "    else:\n",
    "        print(\"No file selected.\")\n",
    "\n",
    "# Create the main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Matching CV with Job Descriptions\")\n",
    "root.geometry(\"800x500\")\n",
    "\n",
    "# Create the canvas\n",
    "canvas = tk.Canvas(root, width=800, height=500, bg=\"white\")\n",
    "canvas.pack()\n",
    "\n",
    "# Create the \"Upload Resume\" button\n",
    "upload_button = tk.Button(canvas, text=\"Upload Resume\", bg=\"blue\", fg=\"white\", font=(\"Arial\", 14), command=upload_resume)\n",
    "upload_button_window = canvas.create_window(400, 250, window=upload_button, anchor=\"center\")\n",
    "\n",
    "# Message label\n",
    "message_label = tk.Label(canvas, text=\"\", font=(\"Arial\", 12))\n",
    "message_label_window = canvas.create_window(400, 300, window=message_label, anchor=\"center\")\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "import re\n",
    "\n",
    "def tokenizer(text):\n",
    "    # the R.E removes any letter that is not an alphabet (a-zA-Z)\n",
    "    text = re.sub(\"[^a-zA-Z]+\", \" \", text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def indexer(jobs, stopwords_path):\n",
    "    global index\n",
    "\n",
    "    c = 0\n",
    "    for x, jd in enumerate(jobs):\n",
    "        tokens = []\n",
    "        # read and lowercase data from current job -> pass to tokenizer()\n",
    "        data = tokenizer(jd)\n",
    "        for i in data:\n",
    "            tokens.append(i)\n",
    "\n",
    "        with open(os.path.join(stopwords_path, 'stopwords.txt'), 'r') as f:\n",
    "            stop = [line.strip() for line in f]\n",
    "\n",
    "        filename = x\n",
    "\n",
    "        for word in tokens:\n",
    "            if word not in stop:\n",
    "                word = ps.stem(word)\n",
    "                if word not in index:\n",
    "                    index[word] = {} \n",
    "                    index[word][filename] = 1\n",
    "                else:\n",
    "                    if filename not in index[word]:\n",
    "                        index[word][filename] = 1\n",
    "                    else:\n",
    "                        index[word][filename] += 1\n",
    "        \n",
    "\n",
    "    # sorting the index by tokens\n",
    "    index = sorted(index.items())\n",
    "    index = dict(index)\n",
    "\n",
    "    for i in range(len(jobs)):\n",
    "        dvectors[i] = [0]*len(index)\n",
    "    \n",
    "    for i, k in enumerate(index):\n",
    "        for key in dvectors.keys(): \n",
    "            if key in index[k]:\n",
    "                df = len(index[k])\n",
    "                idf = math.log(len(dvectors) / df, 10)\n",
    "                score = idf*index[k][key]\n",
    "                dvectors[key][i] = score\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # index is the global dictionary\n",
    "    index = {}\n",
    "    # dvectors contains all document vectors\n",
    "    dvectors = {}\n",
    "    stop = set()\n",
    "\n",
    "    df = pd.read_csv(\"./dataset/data.csv\")\n",
    "    jd = df['Job Description'].tolist()\n",
    "    companies = df['company'].tolist()\n",
    "    positions = df['position'].tolist()\n",
    "    docID = df['docid'].tolist()\n",
    "\n",
    "    # path to the corpus folder\n",
    "    stopwords_path = '.\\\\stopwords'\n",
    "\n",
    "    # stemmer\n",
    "    ps = PorterStemmer()\n",
    "    # all the preprocessing is done inside the indexer()\n",
    "    indexer(jd, stopwords_path)\n",
    "    index = dict(sorted(index.items()))\n",
    "\n",
    "    print(f\"Index created with {len(index)} unique terms!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pdfminer\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from io import StringIO\n",
    "\n",
    "def pdfparser(data):\n",
    "    with open(data, 'rb') as fp:\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        retstr = StringIO()\n",
    "        codec = 'utf-8'\n",
    "        laparams = LAParams()\n",
    "        device = TextConverter(rsrcmgr, retstr, laparams=laparams)\n",
    "        # Create a PDF interpreter object.\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        # Process each page contained in the document.\n",
    "\n",
    "        for page in PDFPage.get_pages(fp):\n",
    "            interpreter.process_page(page)\n",
    "            data =  retstr.getvalue()\n",
    "\n",
    "    return data\n",
    "\n",
    "x = \"./dataset/\"\n",
    "x += uploaded_filename\n",
    "parsed_text = pdfparser(x)\n",
    "with open('resumeconverted.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(parsed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(vector):\n",
    "    s = 0\n",
    "    for i in vector:\n",
    "        s += i ** 2\n",
    "    return(math.sqrt(s))\n",
    "\n",
    "def calculateCos(query):\n",
    "  result = []\n",
    "  qDist = dist(query)\n",
    "  if (qDist == 0):\n",
    "    return result\n",
    "\n",
    "\n",
    "  for doc in dvectors.keys():\n",
    "    cos = 0\n",
    "    dotProduct = 0\n",
    "    docVector = None\n",
    "\n",
    "    docVector = dvectors[doc]\n",
    "\n",
    "    if docVector is not None:\n",
    "      docDist = dist(docVector)\n",
    "\n",
    "      for i in range(0, len(index)):\n",
    "        if (query[i] == 0 or docVector[i] == 0):\n",
    "          continue\n",
    "        else:\n",
    "          dotProduct = dotProduct + (query[i] * docVector[i])\n",
    "      cos = dotProduct / (qDist * docDist)\n",
    "\n",
    "    if (cos > 0.08):\n",
    "      result.append((doc, cos))\n",
    "\n",
    "  result = sorted(result, key=lambda x: -x[1])\n",
    "\n",
    "  with open(\"result.txt\", \"w\") as f:\n",
    "    for i in result:\n",
    "        temp = \"\"\n",
    "        for j in i:\n",
    "            temp += str(j)\n",
    "            temp += \",\"\n",
    "        temp = temp[:-1]\n",
    "        temp += \"\\n\"\n",
    "        f.write(temp)\n",
    "\n",
    "  return result\n",
    "\n",
    "def processQuery(query_tokens): \n",
    "    for i in range(0, len(query_tokens)):\n",
    "        query_tokens[i] = ps.stem(query_tokens[i])\n",
    "    \n",
    "    qvector = [0]*len(index)\n",
    "    qDict = {}\n",
    "\n",
    "    for words in query_tokens:\n",
    "        if(words not in qDict):\n",
    "            qDict[words] = 1\n",
    "        else:\n",
    "            qDict[words] += 1\n",
    "\n",
    "    for i, key in enumerate(index):\n",
    "        if(key in qDict):\n",
    "            df = len(index[key])\n",
    "            idf = math.log(len(dvectors) / df, 10)\n",
    "            score = idf * qDict[key]\n",
    "            qvector[i] = score\n",
    "    \n",
    "    return qvector\n",
    "\n",
    "\n",
    "with open(\"resumeconverted.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read().lower()\n",
    "    query_tokens = tokenizer(data)\n",
    "\n",
    "queryVector = processQuery(query_tokens)\n",
    "result = calculateCos(queryVector)\n",
    "\n",
    "if result: \n",
    "  for i in result:\n",
    "    print(i)\n",
    "else:\n",
    "   print(\"[]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "\n",
    "def show_job_description(index):\n",
    "    position_index, _ = result[index]\n",
    "    company_label.config(text=f\"Company: {companies[position_index]}\")\n",
    "    description_text.config(state=tk.NORMAL)\n",
    "    description_text.delete(\"1.0\", tk.END)\n",
    "    description_text.insert(tk.END, jd[position_index])\n",
    "    description_text.config(state=tk.DISABLED)\n",
    "\n",
    "def create_sidebar(root):\n",
    "    sidebar = tk.Frame(root, width=200, bg=\"gray\")\n",
    "    sidebar.pack(side=tk.LEFT, fill=tk.Y)\n",
    "\n",
    "    for i, _ in enumerate(result):\n",
    "        position_index, _ = result[i]\n",
    "        position = positions[position_index]\n",
    "        button = tk.Button(sidebar, text=f\"Position: {position}\",\n",
    "                           command=lambda i=i: show_job_description(i))\n",
    "        button.pack(fill=tk.X)\n",
    "\n",
    "def create_main_window(root):\n",
    "    global company_label, description_text\n",
    "    main_window = tk.Frame(root)\n",
    "    main_window.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "\n",
    "    company_label = tk.Label(main_window, text=\"Company:\", font=(\"Helvetica\", 12, \"bold\"))\n",
    "    company_label.pack(anchor=tk.W, padx=10, pady=10)\n",
    "\n",
    "    description_text = tk.Text(main_window, wrap=tk.WORD, font=(\"Helvetica\", 10), height=20, width=50)\n",
    "    description_text.pack(fill=tk.BOTH, expand=False, padx=10, pady=5)  # Set expand to False\n",
    "\n",
    "    description_text.config(state=tk.DISABLED)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    root = tk.Tk()\n",
    "    root.title(\"Job Descriptions\")\n",
    "\n",
    "    create_sidebar(root)\n",
    "    create_main_window(root)\n",
    "\n",
    "    root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
